model:
  pretrained_model: QizhiPei/biot5-base
  out_dir: biot5_lora
  max_len: 256
  batch_size: 2
  lr: 0.0006809721777249526
  n_epochs: 5
  # Set to True on CUDA
  quantization: False
  use_cpu: True
lora:
  enabled: True
  r: 17
  alpha: 38
  dropout: 0.07661697140376068
data:
  subset: 1500 # None for all data
  dataset: derm_qa
  # dataset: medXpert
  # dataset: no_robots